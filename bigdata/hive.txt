#Setup Hive and Bee

#Pull the hive image from docker hub
docker pull apache/hive:4.0.0-alpha-1

# you will run the hive server on port 10002. You will name the server instance myhiveserver. 
# We will mount the local data folder in the hive server as hive_custom_data. 
# This would mean that the whole data folder that you created locally, along with anything 
# you add in the data folder, is copied into the container under the directory 
# hive_custom_data.
docker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 -v /home/project/data:/hive_custom_data --name myhiveserver apache/hive:4.0.0-alpha-1

# to access beeline. This is a SQL cli to run hive queries
docker exec -it myhiveserver beeline -u 'jdbc:hive2://localhost:10000/'

create table Employee(emp_id string, emp_name string, salary  int)  row format delimited fields terminated by ',' ;
show tables;
LOAD DATA INPATH '/hive_custom_data/emp.csv' INTO TABLE Employee;
SELECT * FROM employee;

# To quit from the beehive prompt: ctrl+D
# Hive internally uses MapReduce to process and analyze data. When you execute a Hive query, it generates MapReduce jobs that run on the Hadoop cluster.

#Run the Map reduce application for wordcount on data.txt and store the output in /user/root/output
bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount data.txt output

#Run the following command to see the word count output.
cat  output/part-r-00000

